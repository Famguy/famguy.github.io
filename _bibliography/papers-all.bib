---
---

@article{ao2025sonogym,
  title={SonoGym: High Performance Simulation for Challenging Surgical Tasks with Robotic Ultrasound},
  author={Ao, Yunke and Moghani*, Masoud and Mittal*, Mayank and Prajapat, Manish and Wu, Luohong and Giraud, Frederic and Carrillo, Fabio and Krause, Andreas and F{\"u}rnstahl, Philipp},
  journal={arXiv preprint arXiv:2507.01152},
  booktitle={NeurIPS},
  abstract={We present SonoGym, a scalable simulation platform for robotic ultrasound, enabling parallel simulation across tens to hundreds of environments. Our framework supports realistic and real-time simulation of ultrasound data from CT-derived 3D models of the anatomy through both a physics-based and a Generative Adversarial Network (GAN) approach. Our framework enables the training of deep reinforcement learning (DRL) and recent imitation learning agents (IL) (vision transformers and diffusion policies) for ultrasound-guided navigation, anatomy reconstruction and surgery. We believe our simulation can facilitate research in robot learning approaches for such challenging robotic surgery applications. Future research directions include improving ultrasound simulation quality and diversity, modeling soft tissue deformation, scaling to larger patient populations, improving generalization over different patients, and validation with real systems in clinical settings.},
  arxiv={2507.01152},
  year={2025},
  abbr= {sonogym},
  html={https://sonogym.github.io/},
}


@inproceedings{cathomen2025divide,
  title={Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors},
  author={Cathomen, Rafael and Mittal, Mayank and Vlastelica, Marin and Hutter, Marco},
  booktitle={CoRL},
  year={2025},
  arxiv={2508.19953},
  abstract={Unsupervised Skill Discovery (USD) allows agents to autonomously learn diverse behaviors without task-specific rewards. While recent USD methods have shown promise, their application to real-world robotics remains under-explored. We propose a modular USD framework that employs user-defined factorization of the state space to learn disentangled skill representations. Different skill discovery algorithms (METRA, DIAYN) are assigned to each factor based on the desired behavior. We introduce symmetry-based inductive biases tailored to individual factors and incorporate a style factor with regularization penalties to promote safe and robust behaviors. We evaluate our framework using a quadrupedal robot (ANYmal-D) and demonstrate zero-shot transfer of learned skills to real hardware. Our results show that factorization and symmetry lead to structured, human-interpretable behaviors, while the style factor enhances safety and diversity. The learned skills perform on par with oracle policies on downstream navigation tasks.},
  html={https://leggedrobotics.github.io/d3-skill-discovery/},
  abbr= {d3-skill-discovery},
  award={Oral (top 6%)},
}

@inproceedings{portela2024whole,
  title={Whole-Body End-Effector Pose Tracking},
  author={Portela, Tifanny and Cramariuc, Andrei and Mittal, Mayank and Hutter, Marco},
  abstract={Combining manipulation with the mobility of legged robots is essential for a wide range of robotic applications. However, integrating an arm with a mobile base significantly increases the system's complexity, making precise end-effector control challenging. Existing model-based approaches are often constrained by their modeling assumptions, leading to limited robustness. Meanwhile, recent Reinforcement Learning (RL) implementations restrict the arm's workspace to be in front of the robot or track only the position to obtain decent tracking accuracy. In this work, we address these limitations by introducing a whole-body RL formulation for end-effector pose tracking in a large workspace on rough, unstructured terrains. Our proposed method involves a terrain-aware sampling strategy for the robot's initial configuration and end-effector pose commands, as well as a game-based curriculum to extend the robot's operating range. We validate our approach on the ANYmal quadrupedal robot with a six DoF robotic arm. Through our experiments, we show that the learned controller achieves precise command tracking over a large workspace and adapts across varying terrains such as stairs and slopes. On deployment, it achieves a pose-tracking error of 2.64 cm and 3.64 degrees, outperforming existing competitive baselines.},
  booktitle={ICRA},
  journal = {Proceedings of the International Conference on Robotics and Automation (ICRA)},
  year={2025},
  arxiv={2409.16048},
  abbr= {whole-body-ee-pose},
}

@inproceedings{dadiotis2025dynamic,
  title={Dynamic Object Goal Pushing with Mobile Manipulators through Model-Free Constrained Reinforcement Learning},
  author={Dadiotis, Ioannis, and Mittal, Mayank, and Tsagarakis, Nikos, and Hutter, Marco},
  abstract={Non-prehensile pushing to move and reorient objects to a goal is a versatile loco-manipulation skill. In the real world, the object's physical properties and friction with the floor contain significant uncertainties, which makes the task challenging for a mobile manipulator. In this paper, we develop a learning-based controller for a mobile manipulator to move an unknown object to a desired position and yaw orientation through a sequence of pushing actions. The proposed controller for the robotic arm and the mobile base motion is trained using a constrained Reinforcement Learning (RL) formulation. We demonstrate its capability in experiments with a quadrupedal robot equipped with an arm. The learned policy achieves a success rate of 91.35% in simulation and at least 80% on hardware in challenging scenarios. Through our extensive hardware experiments, we show that the approach demonstrates high robustness against unknown objects of different masses, materials, sizes, and shapes. It reactively discovers the pushing location and direction, thus achieving contact-rich behavior while observing only the pose of the object. Additionally, we demonstrate the adaptive behavior of the learned policy towards preventing the object from toppling.},
  booktitle={ICRA},
  journal = {Proceedings of the International Conference on Robotics and Automation (ICRA)},
  arxiv={2502.01546},
  year={2025},
  abbr= {dynamic-object-push},
}

@inproceedings{sleiman2024guided,
  title={Guided Reinforcement Learning for Robust Multi-Contact Loco-Manipulation},
  author={Sleiman*, Jean-Pierre, and Mittal*, Mayank and Hutter, Marco},
  abstract={Reinforcement learning (RL) often necessitates a meticulous Markov Decision Process (MDP) design tailored to each task. This work aims to address this challenge by proposing a systematic approach to behavior synthesis and control for multi-contact loco-manipulation tasks, such as navigating spring-loaded doors and manipulating heavy dishwashers. We define a task-independent MDP to train RL policies using only a single demonstration per task generated from a model-based trajectory optimizer. Our approach incorporates an adaptive phase dynamics formulation to robustly track the demonstrations while accommodating dynamic uncertainties and external disturbances. We compare our method against prior motion imitation RL works and show that the learned policies achieve higher success rates across all considered tasks. These policies learn recovery maneuvers that are not present in the demonstration, such as re-grasping objects during execution or dealing with slippages. Finally, we successfully transfer the policies to a real robot, demonstrating the practical viability of our approach.},
  booktitle={CoRL},
  journal = {Proceedings of the PMLR Conference on Robot Learning},
  arxiv={2410.13817},
  year={2024},
  abbr= {guided-rl-mcp},
  html= {https://leggedrobotics.github.io/guided-rl-locoma},
  award={Oral (top 6%)},
}

@inproceedings{stolle2024perceptivepedipulate,
  title={Perceptive Pedipulation with Local Obstacle Avoidance},
  author={Stolle, Jonas, and Arm, Philip and Mittal, Mayank and Hutter, Marco},
  abstract={Pedipulation leverages the feet of legged robots for mobile manipulation, eliminating the need for dedicated robotic arms. While previous works have showcased blind and task-specific pedipulation skills, they fail to account for static and dynamic obstacles in the environment. To address this limitation, we introduce a reinforcement learning-based approach to train a whole-body obstacle-aware policy that tracks foot position commands while simultaneously avoiding obstacles. Despite training the policy in only five different static scenarios in simulation, we show that it generalizes to unknown environments with different numbers and types of obstacles. We analyze the performance of our method through a set of simulation experiments and successfully deploy the learned policy on the ANYmal quadruped, demonstrating its capability to follow foot commands while navigating around static and dynamic obstacles.},
  booktitle={ICHR},
  journal = {Proceedings of the IEEE International Conference on Humanoid Robots (ICHR)},
  arxiv={2409.07195},
  year={2024},
  abbr= {perceptive-pedipulate},
  html= {https://sites.google.com/leggedrobotics.com/perceptive-pedipulation},
  award={Best Interactive Poster Finalist},
}

@inproceedings{mittal2024symmetry,
  title={Symmetry Considerations for Learning Task Symmetric Robot Policies},
  author={Mittal*, Mayank and Rudin*, Nikita and Klemm, Victor and Allshire, Arthur and Hutter, Marco},
  abstract={Symmetry is a fundamental aspect of many real-world robotic tasks. However, current deep reinforcement learning (DRL) approaches can seldom harness and exploit symmetry effectively. Often, the learned behaviors fail to achieve the desired transformation invariances and suffer from motion artifacts. For instance, a quadruped may exhibit different gaits when commanded to move forward or backward, even though it is symmetrical about its torso. This issue becomes further pronounced in high-dimensional or complex environments, where DRL methods are prone to local optima and fail to explore regions of the state space equally. Past methods on encouraging symmetry for robotic tasks have studied this topic mainly in a single-task setting, where symmetry usually refers to symmetry in the motion, such as the gait patterns. In this paper, we revisit this topic for goal-conditioned tasks in robotics, where symmetry lies mainly in task execution and not necessarily in the learned motions themselves. In particular, we investigate two approaches to incorporate symmetry invariance into DRL -- data augmentation and mirror loss function. We provide a theoretical foundation for using augmented samples in an on-policy setting. Based on this, we show that the corresponding approach achieves faster convergence and improves the learned behaviors in various challenging robotic tasks, from climbing boxes with a quadruped to dexterous manipulation.},
  booktitle={ICRA},
  journal = {Proceedings of the International Conference on Robotics and Automation (ICRA)},
  arxiv={2403.04359},
  year={2024},
  abbr= {symmetry},
}

@inproceedings{arm2024pedipulate,
  title={Pedipulate: Enabling Manipulation Skills using a Quadruped Robot's Leg},
  author={Arm, Philip and Mittal, Mayank and Kolvenbach, Hendrik and Hutter, Marco},
  abstract={Legged robots have the potential to become vital in maintenance, home support, and exploration scenarios. In order to interact with and manipulate their environments, most legged robots are equipped with a dedicated robot arm, which means additional mass and mechanical complexity compared to standard legged robots. In this work, we explore pedipulation - using the legs of a legged robot for manipulation. By training a reinforcement learning policy that tracks position targets for one foot, we enable a dedicated pedipulation controller that is robust to disturbances, has a large workspace through whole-body behaviors, and can reach far-away targets with gait emergence, enabling loco-pedipulation. By deploying our controller on a quadrupedal robot using teleoperation, we demonstrate various real-world tasks such as door opening, sample collection, and pushing obstacles. We demonstrate load carrying of more than 2.0 kg at the foot. Additionally, the controller is robust to interaction forces at the foot, disturbances at the base, and slippery contact surfaces.},
  booktitle={ICRA},
  journal = {Proceedings of the International Conference on Robotics and Automation (ICRA)},
  arxiv={2402.10837},
  year={2024},
  abbr= {pedipulate},
  video= {https://youtu.be/GD4WyJPXQtU},
  html= {https://sites.google.com/leggedrobotics.com/pedipulate},
  selected={true},
}

@inproceedings{roth2023viplanner,
  title={ViPlanner: Visual Semantic Imperative Learning for Local Navigation},
  author={Roth, Pascal and Nubert, Julian and Yang, Fan and Mittal, Mayank and Hutter, Marco},
  abstract={Real-time path planning in outdoor environments still challenges modern robotic systems due to differences in terrain traversability, diverse obstacles, and the necessity for fast decision-making. Established approaches have primarily focused on geometric navigation solutions, which work well for structured geometric obstacles but have limitations regarding the semantic interpretation of different terrain types and their affordances. Moreover, these methods fail to identify traversable geometric occurrences, such as stairs. To overcome these issues, we introduce ViPlanner, a learned local path planning approach that generates local plans based on geometric and semantic information. The system is trained using the Imperative Learning paradigm, for which the network weights are optimized end-to-end based on the planning task objective. This optimization uses a differentiable formulation of a semantic costmap, which enables the planner to distinguish between the traversability of different terrains and accurately identify obstacles. The semantic information is represented in 30 classes using an RGB colorspace that can effectively encode the multiple levels of traversability. We show that the planner can adapt to diverse real-world environments without requiring any real-world training. In fact, the planner is trained purely in simulation, enabling a highly scalable training data generation. Experimental results demonstrate resistance to noise, zero-shot sim-to-real transfer, and a decrease of 38.02% in terms of traversability cost compared to purely geometric-based approaches.},
  booktitle={ICRA},
  journal = {Proceedings of the International Conference on Robotics and Automation (ICRA)},
  arxiv={2310.00982},
  year={2024},
  abbr= {viplanner},
  video= {https://www.youtube.com/watch?v=6FEgU0kYqlM},
  code= {https://github.com/leggedrobotics/viplanner},
}

@inproceedings{mittal2023orbit,
  title={ORBIT: A Unified Simulation Framework for Interactive Robot Learning Environments},
  author={Mittal, Mayank and Yu, Calvin and Yu, Qinxi and Liu, Jingzhou and Rudin, Nikita and Hoeller, David and others},
  abstract={We present ORBIT, a unified and modular framework for robot learning powered by NVIDIA Isaac Sim. It offers a modular design to easily and efficiently create robotic environments with photo-realistic scenes and fast and accurate rigid and deformable body simulation. With ORBIT, we provide a suite of benchmark tasks of varying difficulty -- from single-stage cabinet opening and cloth folding to multi-stage tasks such as room reorganization. To support working with diverse observations and action spaces, we include fixed-arm and mobile manipulators with different physically-based sensors and motion generators. ORBIT allows training reinforcement learning policies and collecting large demonstration datasets from hand-crafted or expert solutions in a matter of minutes by leveraging GPU-based parallelization. In summary, we offer an open-sourced framework that readily comes with 16 robotic platforms, 4 sensor modalities, 10 motion generators, more than 20 benchmark tasks, and wrappers to 4 learning libraries. With this framework, we aim to support various research areas, including representation learning, reinforcement learning, imitation learning, and task and motion planning. We hope it helps establish interdisciplinary collaborations in these communities, and its modularity makes it easily extensible for more tasks and applications in the future.},
  arxiv={2301.04195},
  booktitle = {IEEE RA-L},
  journal={IEEE Robotics and Automation Letters},
  year={2023},
  abbr={orbit},
  html={https://isaac-orbit.github.io/},
  code={https://github.com/NVIDIA-Omniverse/orbit},
  selected={true},
}

@inproceedings{2202.12385,
Author = {Jia-Ruei Chiu and Jean-Pierre Sleiman and Mayank Mittal and Farbod Farshidian and Marco Hutter},
Title = {A Collision-Free MPC for Whole-Body Dynamic Locomotion and Manipulation},
Year = {2022},
abstract = {In this paper, we present a real-time whole-body planner for collision-free legged mobile manipulation. We enforce both self-collision and environment-collision avoidance as soft constraints within a Model Predictive Control (MPC) scheme that solves a multi-contact optimal control problem. By penalizing the signed distances among a set of representative primitive collision bodies, the robot is able to safely execute a variety of dynamic maneuvers while preventing any self-collisions. Moreover, collision-free navigation and manipulation in both static and dynamic environments are made viable through efficient queries of distances and their gradients via a euclidean signed distance field. We demonstrate through a comparative study that our approach only slightly increases the computational complexity of the MPC planning. Finally, we validate the effectiveness of our framework through a set of hardware experiments involving dynamic mobile manipulation tasks with potential collisions, such as locomotion balancing with the swinging arm, weight throwing, and autonomous door opening.},
booktitle = {ICRA},
journal = {Proceedings of the International Conference on Robotics and Automation (ICRA)},
abbr= {alma_self_collision},
arxiv = {2202.12385},
video = {https://www.youtube.com/watch?v=m3rJWJVzYuY},
}

@inproceedings{2108.09779,
  Title={Transferring Dexterous Manipulation from GPU Simulation to a Remote Real-World TriFinger},
  Author={Allshire, Arthur and Mittal, Mayank and Lodaya, Varun and Makoviychuk, Viktor and Makoviichuk, Denys and Widmaier, Felix and W{\"u}thrich, Manuel and Bauer, Stefan and Handa, Ankur and Garg, Animesh},
  booktitle = {IROS},
  abstract = {We present a system for learning a challenging dexterous manipulation task involving moving a cube to an arbitrary 6-DoF pose with only 3-fingers trained with NVIDIA's IsaacGym simulator. We show empirical benefits, both in simulation and sim-to-real transfer, of using keypoints as opposed to position+quaternion representations for the object pose in 6-DoF for policy observations and in reward calculation to train a model-free reinforcement learning agent. By utilizing domain randomization strategies along with the keypoint representation of the pose of the manipulated object, we achieve a high success rate of 83% on a remote TriFinger system maintained by the organizers of the Real Robot Challenge. With the aim of assisting further research in learning in-hand manipulation, we make the codebase of our system, along with trained checkpoints that come with billions of steps of experience available, available publicly.},
  arxiv={2108.09779},
  journal = {Proceedings of the International Conference on Intelligent Robots and Systems (IROS)},
  Year={2022},
  abbr={trifinger},
  html={https://s2r2-ig.github.io/},
  code={https://github.com/pairlab/leibnizgym}
}

@inproceedings{2002.10451,
Author = {Mayank Mittal and David Hoeller and Farbod Farshidian and Marco Hutter and Animesh Garg},
Title = {Articulated Object Interaction in Unknown Scenes with Whole-Body Mobile Manipulation},
abstract = {A kitchen assistant needs to operate human-scale objects, such as cabinets and ovens, in unmapped environments with dynamic obstacles. Autonomous interactions in such real-world environments require integrating dexterous manipulation and fluid mobility. While mobile manipulators in different form-factors provide an extended workspace, their real-world adoption has been limited. This limitation is in part due to two main reasons: 1) inability to interact with unknown human-scale objects such as cabinets and ovens, and 2) inefficient coordination between the arm and the mobile base. Executing a high-level task for general objects requires a perceptual understanding of the object as well as adaptive whole-body control among dynamic obstacles. In this paper, we propose a two-stage architecture for autonomous interaction with large articulated objects in unknown environments. The first stage uses a learned model to estimate the articulated model of a target object from an RGB-D input and predicts an action-conditional sequence of states for interaction. The second stage comprises of a whole-body motion controller to manipulate the object along the generated kinematic plan. We show that our proposed pipeline can handle complicated static and dynamic kitchen settings. Moreover, we demonstrate that the proposed approach achieves better performance than commonly used control methods in mobile manipulation.},
booktitle = {IROS},
journal = {Proceedings of the International Conference on Intelligent Robots and Systems (IROS)},
Year={2022},
arxiv = {2103.10534},
html = {https://www.pair.toronto.edu/articulated-mm/},
abbr= {articulated_mm},
selected={true},
}

@preprint{2002.10451,
Author = {Mayank Mittal and Marco Gallieri and Alessio Quaglino and  Seyed Sina Mirrazavi Salehian and Jan Koutnik},
Title = {Neural Lyapunov Model Predictive Control},
Year = {2020},
abstract = {This paper presents \emph{Neural Lyapunov MPC}, an algorithm to alternately train a Lyapunov neural network and a stabilising constrained Model Predictive Controller (MPC), given a neural network model of the system dynamics. This extends recent works on Lyapunov networks to be able to train solely from expert demonstrations of one-step transitions. The learned Lyapunov network is used as the value function for the MPC in order to guarantee stability and extend the stable region. Formal results are presented on the existence of a set of MPC parameters, such as discount factors, that guarantees stability with a horizon as short as one. Robustness margins are also discussed and existing performance bounds on value function MPC are extended to the case of imperfect models. The approach is tested on unstable non-linear continuous control tasks with hard constraints. Results demonstrate that, when a neural network trained on short sequences is used for predictions, a one-step horizon Neural Lyapunov MPC can successfully reproduce the expert behaviour and significantly outperform longer horizon MPCs.},
booktitle = {Review},
journal = {},
arxiv = {2002.10451},
abbr= {lyap_mpc}
}

@inproceedings{2001.01234,
Author = {Andrei Cramariuc and Aleksandar Petrov and Rohit Suri and Mayank Mittal and Roland Siegwart and Cesar Cadena},
Title = {Learning Camera Miscalibration Detection},
Year = {2020},
abstract = {Self-diagnosis and self-repair are some of the key challenges in deploying robotic platforms for long-term real-world applications. One of the issues that can occur to a robot is miscalibration of its sensors due to aging, environmental transients or external disturbances. Precise calibration lies at the core of a variety of applications, due to the need to accurately perceive the world. However, while a lot of work has focused on calibrating the sensors, not much has been done towards identifying when a sensor needs to be recalibrated. In this paper, we focus on a data-driven approach to learn the detection of miscalibration in vision sensors, specifically RGB cameras. Our contributions include a proposed miscalibration metric for RGB cameras and a novel semi-synthetic dataset generation pipeline based on this metric. Additionally, by training a deep convolutional neural network, we demonstrate the effectiveness of our pipeline to identify whether a recalibration of the camera's intrinsic parameters is required or not.},
booktitle = {ICRA},
journal = {Proceedings of the International Conference on Robotics and Automation (ICRA)},
abbr= {miscalib},
arxiv = {2005.11711},
code = {http://github.com/ethz-asl/camera_miscalib_detection}
}

@inproceedings{1906.01304,
Author = {Mayank Mittal and Rohit Mohan and Wolfram Burgard and Abhinav Valada},
Title = {Vision-based Autonomous UAV Navigation and Landing for Urban Search and Rescue},
Year = {2019},
abstract = {Unmanned Aerial Vehicles (UAVs) equipped with bioradars are a life-saving technology that can enable identification of survivors under collapsed buildings in the aftermath of natural disasters such as earthquakes or gas explosions. However, these UAVs have to be able to autonomously navigate in disaster struck environments and land on debris piles in order to accurately locate the survivors. This problem is extremely challenging as pre-existing maps cannot be leveraged for navigation due to structural changes that may have occurred and existing landing site detection algorithms are not suitable to identify safe landing regions on debris piles. In this work, we present a computationally efficient system for autonomous UAV navigation and landing that does not require any prior knowledge about the environment. We propose a novel landing site detection algorithm that computes costmaps based on several hazard factors including terrain flatness, steepness, depth accuracy, and energy consumption information. We also introduce a first-of-a-kind synthetic dataset of over 1.2 million images of collapsed buildings with groundtruth depth, surface normals, semantics and camera pose information. We demonstrate the efficacy of our system using experiments from a city scale hyperrealistic simulation environment and in real-world scenarios with collapsed buildings.},
booktitle = {ISRR},
journal = {Proceedings of the International Symposium of Robotics Research (ISRR)},
arxiv = {1906.01304},
html = {http://autoland.cs.uni-freiburg.de},
abbr = {vision_uav}
}

@workshop{1809.05700,
Author = {Mayank Mittal and Abhinav Valada and Wolfram Burgard},
Title = {Vision-based Autonomous Landing in Catastrophe-Struck Environments},
Year = {2018},
abstract = {Unmanned Aerial Vehicles (UAVs) equipped with bioradars are a life-saving technology that can enable identification of survivors under collapsed buildings in the aftermath of natural disasters such as earthquakes or gas explosions. However, these UAVs have to be able to autonomously land on debris piles in order to accurately locate the survivors. This problem is extremely challenging as the structure of these debris piles is often unknown and no prior knowledge can be leveraged. In this work, we propose a computationally efficient system that is able to reliably identify safe landing sites and autonomously perform the landing maneuver. Specifically, our algorithm computes costmaps based on several hazard factors including terrain flatness, steepness, depth accuracy and energy consumption information. We first estimate dense candidate landing sites from the resulting costmap and then employ clustering to group neighboring sites into a safe landing region. Finally, a minimum-jerk trajectory is computed for landing considering the surrounding obstacles and the UAV dynamics. We demonstrate the efficacy of our system using experiments from a city scale hyperrealistic simulation environment and in real-world scenarios with collapsed buildings.},
booktitle = {Workshop on Vision-based Drones: What's Next?},
maintitle = {IROS},
arxiv = {1809.05700},
video = {https://www.youtube.com/watch?v=6FEgU0kYqlM},
pdf = {papers/mittal18irosws.pdf},
abbr = {landing}
}
