<!--<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="seal_icon.png">
  <title>Sagun Pai</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Sagun Pai</name>
        </p>
        <p>I am a senior research scientist at <a href="https://research.google.com/">Google Research</a>, where I work on computer vision and computational photography. At Google I've worked on <a href="http://googleresearch.blogspot.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="http://googleresearch.blogspot.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://research.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>.
        </p>
        <p>
          I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've spent time at <a href="https://en.wikipedia.org/wiki/Google_X">Google[x]</a>, <a href="http://groups.csail.mit.edu/vision/welcome/">MIT CSAIL</a>, <a href="http://www.captricity.com/">Captricity</a>, <a href="https://www.nasa.gov/ames">NASA Ames</a>, <a href="http://www.google.com/">Google NYC</a>, the <a href="http://mrl.nyu.edu/">NYU MRL</a>, <a href="http://www.nibr.com/">Novartis</a>, and <a href="http://www.astrometry.net/">Astrometry.net</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.
        </p>
        <p align=center>
          <a href="mailto:jonbarron@gmail.com">Email</a> &nbsp/&nbsp
          <a href="cv.pdf">CV</a> &nbsp/&nbsp
          <a href="JonBarron-bio.txt">Biography</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp
          <a href="http://www.linkedin.com/in/jonathanbarron/"> LinkedIn </a>
        </p>
        </td>
        <td width="33%">
        <img src="JonBarron_circle.jpg">
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p>
          I'm interested in computer vision, machine learning, statistics, optimization, image processing, virtual reality, and computational photography. Much of my research is about inferring the physical world (shape, depth, motion, paint, light, colors, etc) from images. I have also worked in astronomy and biology. Representative papers are <span class="highlight">highlighted</span>.
          </p>
        </td>
      </tr>
      </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="25%">
        <div class="one">
        <div class="two" id = 'aperture_image'><img src='aperture_after.jpg'></div>
        <img src='aperture_before.jpg'>
        </div>
        <script type="text/javascript">
        function aperture_start() {
        document.getElementById('aperture_image').style.opacity = "1";
        }
        function aperture_stop() {
        document.getElementById('aperture_image').style.opacity = "0";
        }
        aperture_stop()
        </script>
      </td>
      <td valign="top" width="75%">
	  <a href="https://arxiv.org/abs/1711.07933">
            <papertitle>Aperture Supervision for Monocular Depth Estimation</papertitle>
	  </a>
	  <br>
          <a href="https://people.eecs.berkeley.edu/~pratul/">Pratul P. Srinivasan</a>,
	  <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
	  <a href="http://people.csail.mit.edu/nwadhwa/">Neal Wadhwa</a>,
	  <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>,
	  <strong>Jonathan T. Barron</strong> <br>
        <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018 <br>
        <p></p>
        <p>Varying a camera's aperture provides a supervisory signal that can teach a neural network to do monocular depth estimation.</p>
      </td>
    </tr>
		
    <tr onmouseout="deepburst_stop()" onmouseover="deepburst_start()" >
      <td width="25%">
        <div class="one">
        <div class="two" id = 'deepburst_image'><img src='deepburst_after.png'></div>
        <img src='deepburst_before.png'>
        </div>
        <script type="text/javascript">
        function deepburst_start() {
        document.getElementById('deepburst_image').style.opacity = "1";
        }
        function deepburst_stop() {
        document.getElementById('deepburst_image').style.opacity = "0";
        }
        deepburst_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <a href="https://arxiv.org/abs/1712.02327">
        <papertitle>Burst Denoising with Kernel Prediction Networks</papertitle>
        </a>
        <br>
        <a href="https://people.eecs.berkeley.edu/~bmild/">Ben Mildenhall</a>,
        <strong>Jonathan T. Barron</strong>,
        <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
        Dillon Sharlet,
        <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>,
        Robert Carroll <br>
        <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018 <br>
        <p></p>
        <p>We train a network to predict linear kernels that denoise noisy bursts from cellphone cameras.</p>
      </td>
    </tr>
	  
    <tr onmouseout="friendly_stop()" onmouseover="friendly_start()" >
      <td width="25%">
        <div class="one">
        <div class="two" id = 'friendly_image'><img src='friendly_after.png'></div>
        <img src='friendly_before.png'>
        </div>
        <script type="text/javascript">
        function friendly_start() {
        document.getElementById('friendly_image').style.opacity = "1";
        }
        function friendly_stop() {
        document.getElementById('friendly_image').style.opacity = "0";
        }
        friendly_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://drive.google.com/file/d/1w_0djhL0QgC_fbehnJ0c-J23_kW_420p/view?usp=sharing">
        <papertitle>A Hardware-Friendly Bilateral Solver for Real-Time Virtual Reality Video</papertitle></a><br>
          <a href="https://homes.cs.washington.edu/~amrita/">Amrita Mazumdar</a>, <a href="http://homes.cs.washington.edu/~armin/">Armin Alaghi</a>, <strong>Jonathan T. Barron</strong>, <a href="https://www.cs.unc.edu/~gallup/">David Gallup</a>, <a href="https://homes.cs.washington.edu/~luisceze/">Luis Ceze</a>, <a href="https://homes.cs.washington.edu/~oskin/">Mark Oskin</a>, <a href="http://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a>  <br>
        <em>High-Performance Graphics (HPG)</em>, 2017 <br>
        <a href="https://sampa.cs.washington.edu/projects/vr-hw.html">project page</a>
        <p></p>
        <p>A reformulation of the bilateral solver can be implemented efficiently on GPUs and FPGAs.</p>
      </td>
    </tr>

    <tr onmouseout="hdrnet_stop()" onmouseover="hdrnet_start()" >
      <td width="25%">
        <div class="one">
        <div class="two" id = 'hdrnet_image'><img src='hdrnet_after.jpg'></div>
        <img src='hdrnet_before.jpg'>
        </div>
        <script type="text/javascript">
        function hdrnet_start() {
        document.getElementById('hdrnet_image').style.opacity = "1";
        }
        function hdrnet_stop() {
        document.getElementById('hdrnet_image').style.opacity = "0";
        }
        hdrnet_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://drive.google.com/file/d/1jQY3CTMnLX7PeGUzYLso9H1eCsZyWbwg/view?usp=sharing">
        <papertitle>Deep Bilateral Learning for Real-Time Image Enhancement</papertitle></a><br>
          <a href="http://www.mgharbi.com">Micha&euml;l Gharbi</a>, <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>, <strong>Jonathan T. Barron</strong>, <a href="https://people.csail.mit.edu/hasinoff/">Samuel W. Hasinoff</a>, <a href="http://people.csail.mit.edu/fredo/">Fr&eacute;do Durand </a> <br>
        <em>SIGGRAPH</em>, 2017 <br>
        <a href="https://groups.csail.mit.edu/graphics/hdrnet/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=GAe0qKKQY_I">video</a>
        /
        <a href="GharbiSIGGRAPH2017.bib">bibtex</a>
        /
        <a href="http://news.mit.edu/2017/automatic-image-retouching-phone-0802">p</a><a href="https://www.wired.com/story/googles-new-algorithm-perfects-photos-before-you-even-take-them/">r</a><a href="https://petapixel.com/2017/08/02/new-ai-can-retouch-photos-snap/">e</a><a href="https://www.theverge.com/2017/8/2/16082272/google-mit-retouch-photos-machine-learning">s</a><a href="http://gizmodo.com/clever-camera-app-uses-deep-learning-to-perfectly-retou-1797474282">s</a>
        <p></p>
        <p>By training a deep network in bilateral space we can learn a model for high-resolution and real-time image enhancement.</p>
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src='loss.png'>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/1701.03077">
          <papertitle>A More General Robust Loss Function</papertitle></a><br>
          <strong>Jonathan T. Barron</strong> <br>
          <em>arXiv Preprint</em>, 2017 <br>
	  <p></p>
          <p>A single robust loss function is a superset of many other common robust loss functions.</p>
        </td>
      </tr>

      <tr>
        <td width="25%"><img src="clean_promo.jpg" alt="clean-usnob" width="160" height="160"></td>
        <td width="75%" valign="top">
        <p>
        <p>
          <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
          <papertitle>Cleaning the USNO-B Catalog Through Automatic Detection of Optical Artifacts</papertitle>
          </a>
          <br>
          <strong>Jonathan T. Barron</strong>, <a href="http://stumm.ca/">Christopher Stumm</a>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a><br>
          <em>The Astronomical Journal</em>, 135, 2008
        </p>
        <p>We use computer vision techniques to identify and remove diffraction spikes and reflection halos in the USNO-B Catalog.</p>
        <p>In use at <a href="http://www.astrometry.net">Astrometry.net</a><br>
          <br>
        </p>
        </p>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Course Projects</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td width="25%"><img src="prl.jpg" alt="prl" width="160" height="160"></td>
        <td width="75%" valign="top">
        <p>
          <a href="https://drive.google.com/file/d/13rVuJpcytRdLYCnKpq46g7B7IzSrPQ2P/view?usp=sharing">
          <papertitle>Parallelizing Reinforcement Learning</papertitle>
          </a>
          <br>
          <strong>Jonathan T. Barron</strong>, <a href="http://www.eecs.berkeley.edu/~dsg/">Dave Golland</a>, <a href="http://www.cs.berkeley.edu/~nickjhay/">Nicholas J. Hay</a>, 2009
        <p><br>
          Markov Decision Problems which lie in a low-dimensional latent space can be decomposed, allowing modified RL algorithms to run orders of magnitude faster in parallel.
        </p>
        </p>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Teaching</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td width="25%"><img src="pacman.jpg" alt="pacman" width="160" height="160"></td>
        <td width="75%" valign="center">
        <p>
          <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">
          <papertitle>CS188 - Fall 2010 (GSI)</papertitle>
          </a>
          <br><br>
          <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">
          <papertitle>CS188 - Spring 2011 (GSI)</papertitle>
          </a>
          <br>
        </p>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website"><strong>source code</strong></a>, just add a link back to my website. Send me an email when you're done and I'll link to your new page from here:
          <a href="https://cs.stanford.edu/~poole/">&#10025;</a>
          <a href="http://www.cs.berkeley.edu/~akar/">&#10025;</a>
          <a href="http://www.eecs.berkeley.edu/~biancolin">&#10025;</a>
          <a href="http://www.rossgirshick.info/">&#10025;</a>
	    </font>
        </p>
        </td>
      </tr>
      </table>
      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-7580334-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
    </td>
    </tr>
  </table>
  </body>
</html>
-->

<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US"><head><title>G126: Providing a list of links to all other Web pages | Techniques for WCAG 2.0
              </title><link rel="canonical" href="http://www.w3.org/TR/WCAG20-TECHS/G126.html"/><link rel="stylesheet" type="text/css" href="http://www.w3.org/StyleSheets/TR/2016/W3C-WG-NOTE.css"/><link rel="stylesheet" type="text/css" href="additional.css"/><link rel="stylesheet" type="text/css" href="slicenav.css"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/></head><body class="slices toc-inline"><div id="masthead"><p class="logo"><a href="http://www.w3.org/"><img width="72" height="48" alt="W3C" src="https://www.w3.org/StyleSheets/TR/2016/logos/W3C"/></a></p><p class="collectiontitle"><a href="./">Techniques for WCAG 2.0</a></p></div><div id="skipnav"><p class="skipnav"><a href="#maincontent">Skip to Content (Press Enter)</a></p></div><a name="top"> </a><!-- TOP NAVIGATION BAR --><ul id="navigation"><li><strong><a href="Overview.html#contents" title="Table of Contents">Contents</a></strong></li><li><strong><a href="intro.html" title="Introduction to Techniques for WCAG 2.0"><abbr title="Introduction">Intro</abbr></a></strong></li><li><a title="G125: Providing links to navigate to related Web pages" href="G125.html"><strong>Previous: </strong>
        Technique G125</a></li><li><a title="G127: Identifying a Web page's relationship to a larger collection of Web pages" href="G127.html"><strong>Next: </strong>
        Technique G127</a></li></ul><div class="navtoc"><p>On this page:</p><ul id="navbar"><li><a href="#G126-disclaimer">Important Information about Techniques</a></li><li><a href="#G126-applicability">Applicability</a></li><li><a href="#G126-description">Description</a></li><li><a href="#G126-examples">Examples</a></li><li><a href="#G126-resources">Resources</a></li><li><a href="#G126-related-techs">Related Techniques</a></li><li><a href="#G126-tests">Tests</a></li></ul></div><div class="skiptarget"><a id="maincontent">-</a></div> <h1><a name="G126" id="G126"> </a>G126: Providing a list of links to all other Web pages</h1><div id="G126-disclaimer"><h2>Important Information about Techniques</h2><p>See <a href="http://www.w3.org/TR/2016/NOTE-UNDERSTANDING-WCAG20-20161007/understanding-techniques.html">Understanding Techniques for WCAG Success Criteria</a> for important information about the usage of these informative techniques and how they relate to the normative WCAG 2.0 success criteria. The Applicability section explains the scope of the technique, and the presence of techniques for a specific technology does not imply that the technology can be used in all situations to create content that meets WCAG 2.0.</p></div><div class="applicability"><h2 id="G126-applicability">Applicability</h2><div class="textbody"><p>All technologies that contain links</p></div></div><p class="referenced">This technique relates to:</p><ul><li><a href="http://www.w3.org/TR/2008/REC-WCAG20-20081211/#navigation-mechanisms-mult-loc">
				Success Criterion 2.4.5 (Multiple Ways)</a><ul><li><a href="http://www.w3.org/WAI/WCAG20/quickref/20160105/#navigation-mechanisms-mult-loc">
						How to Meet 2.4.5 (Multiple Ways)
					</a></li><li><a href="http://www.w3.org/TR/2016/NOTE-UNDERSTANDING-WCAG20-20161007/navigation-mechanisms-mult-loc.html">
						Understanding Success Criterion 2.4.5 (Multiple Ways)
					</a></li></ul></li></ul><h2 id="G126-description">Description</h2><div class="textbody"><p>The objective of this technique is to provide a list of links to all the Web pages in the set on each Web page. It is one of a series of techniques for locating content that are sufficient for addressing Success Criterion 2.4.5. 
            This technique is only effective for small sets of Web pages; if the list of links is longer than the rest of the content in the Web page, it may make the Web page more difficult for users to understand and use.</p><div class="note"><p class="prefix"><em>Note: </em>Success Criterion 2.4.1 requires a technique for skipping this list of links.</p></div></div><h2 class="small-head" id="G126-examples">Examples</h2><h3 class="small-head" id="G126-ex1">Example 1</h3><div class="example"><div class="textbody"><p>A family Web site contains home pages for all the members of the family. Each page contains a list of links to the home pages of the other family members.</p></div></div><h3 class="small-head" id="G126-ex2">Example 2</h3><div class="example"><div class="textbody"><p>An electonic book is broken into separate Web pages for each chapter. Each Web page starts with a small table of contents that contains links to all the chapters in the book.</p></div></div><h2 id="G126-resources">Resources</h2><div class="textbody"><p>No resources available for this technique.</p></div><h2 id="G126-related-techs">Related Techniques</h2><div class="textbody"><ul><li><a href="G1.html">G1: Adding a link at the top of each page that goes directly to the main content area</a></li><li><a href="G63.html">G63: Providing a site map</a></li><li><a href="G64.html">G64: Providing a Table of Contents</a></li><li><a href="G123.html">G123: Adding a link at the beginning of a block of repeated content to go to the end of the block</a></li><li><a href="G125.html">G125: Providing links to navigate to related Web pages</a></li></ul></div><h2 id="G126-tests">Tests</h2><div class="textbody"><h3 class="small-head" id="G126-procedure">Procedure</h3><ol class="enumar"><li><p>Check that each Web page contains a list of links to the other Web pages in the site</p></li><li><p>Check that the links in the list lead to the corresponding Web pages.</p></li><li><p>Check that the list contains a link for every Web page in the site.</p></li></ol><h3 class="small-head" id="G126-results">Expected Results</h3><ul><li><p>All of the checks above are true.</p></li></ul><p>If this is a sufficient technique for a success criterion, failing this test procedure does not necessarily mean that the success criterion has not been satisfied in some other way, only that this technique has not been successfully implemented and can not be used to claim conformance.</p></div><!-- BOTTOM NAVIGATION BAR --><ul id="navigationbottom"><li><strong><a href="#top">Top</a></strong></li><li><strong><a href="Overview.html#contents" title="Table of Contents">Contents</a></strong></li><li><strong><a href="intro.html" title="Introduction to Techniques for WCAG 2.0"><abbr title="Introduction">Intro</abbr></a></strong></li><li><a title="G125: Providing links to navigate to related Web pages" href="G125.html"><strong>Previous: </strong>
        Technique G125</a></li><li><a title="G127: Identifying a Web page's relationship to a larger collection of Web pages" href="G127.html"><strong>Next: </strong>
        Technique G127</a></li></ul><div class="footer"><p class="copyright">This Web page is part of <a href="Overview.html">Techniques and Failures for Web Content Accessibility Guidelines 2.0</a> (see the <a href="http://www.w3.org/TR/WCAG20-TECHS/G126.html">latest version of this document</a>). The entire document is also available as a <a href="complete.html">single HTML file</a>. See the <a href="http://www.w3.org/WAI/intro/wcag20">The WCAG 2.0 Documents</a> for an explanation of how this document fits in with other Web Content Accessibility Guidelines (WCAG) 2.0 documents. To send public comments, please follow the <a href="http://www.w3.org/WAI/WCAG20/comments/">Instructions for Commenting on WCAG 2.0 Documents</a>.
 </p><p class="copyright"><a href="http://www.w3.org/Consortium/Legal/ipr-notice#Copyright">Copyright</a> © 2016 <a href="http://www.w3.org/"><acronym title="World Wide Web Consortium">W3C</acronym></a><sup>®</sup> (<a href="http://www.csail.mit.edu/"><acronym title="Massachusetts Institute of Technology">MIT</acronym></a>, <a href="http://www.ercim.eu/"><acronym title="European Research Consortium for Informatics and Mathematics">ERCIM</acronym></a>, <a href="http://www.keio.ac.jp/">Keio</a>, <a href="http://ev.buaa.edu.cn/">Beihang</a>). W3C <a href="http://www.w3.org/Consortium/Legal/ipr-notice#Legal_Disclaimer">liability</a>, <a href="http://www.w3.org/Consortium/Legal/ipr-notice#W3C_Trademarks">trademark</a> and <a href="http://www.w3.org/Consortium/Legal/copyright-documents">document use</a> rules apply.</p></div></body></html>
